# LinkedIn Strategy for SWE Leaders

## Overview

This document defines our LinkedIn strategy targeting engineering managers, directors, and VPs who need visibility into AI-assisted development. LinkedIn is our primary channel for leadership-focused messaging.

**Target Audience:** Engineering managers, tech leads, directors, VPs of Engineering
**Voice:** Thought leadership, strategic, data-driven, peer-to-peer
**Goal:** Position ginko as the solution for observable, coordinated AI collaboration at scale

---

## Audience Profile

### Primary: Engineering Managers & Directors

**Who they are:**
- Managing teams of 5-50 engineers
- Responsible for productivity, quality, and delivery
- Making tooling decisions for their teams
- Accountable for team output to leadership

**Pain points:**
- "How do I know my team is being productive with AI tools?"
- "AI coding is a black box - I can't see what's happening"
- "Rework rates are killing our velocity"
- "Knowledge silos are forming around AI sessions"
- "New hire onboarding isn't improving despite AI tools"

**What they care about:**
- Team visibility and observability
- Reducing rework and waste
- Knowledge sharing and continuity
- Measurable ROI on tooling investments
- Predictable delivery

### Secondary: VPs of Engineering / CTOs

**Who they are:**
- Overseeing multiple teams or entire engineering orgs
- Setting technical strategy and tooling standards
- Reporting to C-suite on engineering efficiency

**Pain points:**
- "Are AI tools actually improving org-wide productivity?"
- "How do we scale AI adoption across teams?"
- "What's the ROI of our AI tooling investment?"

**What they care about:**
- Org-wide metrics and trends
- Standardization and governance
- Strategic advantage from AI adoption
- Risk management

---

## Content Pillars

### 1. Observable AI Collaboration

**Theme:** Making the invisible visible

**Topics:**
- Why AI work is a black box for managers
- Metrics that matter for AI-assisted teams
- From individual productivity to team observability
- The hidden cost of invisible AI decisions

### 2. Team Coordination in the AI Era

**Theme:** Individual productivity vs. team effectiveness

**Topics:**
- AI productivity gains creating coordination losses
- Knowledge silos from ephemeral AI sessions
- Why onboarding is harder despite AI tools
- Patterns for team-wide AI adoption

### 3. Measuring AI ROI

**Theme:** Beyond lines of code

**Topics:**
- Why traditional metrics miss AI impact
- Framework for measuring AI collaboration ROI
- Rework rate as the key metric
- Context recovery time as hidden cost

### 4. Velocity & Estimation

**Theme:** What changes when AI accelerates development

**Topics:**
- Why 6-week estimates take 3 days
- New estimation frameworks for Human+AI teams
- The death of story points (and what replaces them)
- Sustainable velocity in AI-first teams

---

## Post Formats

### Format 1: Insight Post (Most Common)

**Length:** 1200-1500 characters
**Structure:** Hook → Problem → Insight → Takeaway → CTA

**Template:**
```
[Hook - surprising observation or contrarian take]

[Problem description - 2-3 sentences with specifics]

[Insight - what most people miss]

[Key points - bullet list of 3-4 items]

[Takeaway - one clear lesson]

[Soft CTA - question or link]
```

### Format 2: Data/Case Study Post

**Length:** 1000-1300 characters
**Structure:** Headline data → Context → Analysis → Implication

**Template:**
```
[Headline metric or result]

[Context - what we did, what happened]

[The breakdown - bullet points with data]

[What this means for you]

[Question to invite discussion]
```

### Format 3: Framework Post

**Length:** 1300-1500 characters
**Structure:** Problem → Framework intro → Steps → Application

**Template:**
```
[Problem most leaders face]

Here's a framework that helps:

[Framework name or description]

Step 1: [Action]
Step 2: [Action]
Step 3: [Action]

[How to apply this to your team]

[Link to deeper resource]
```

---

## Pre-Written Posts

### Post 1: The Hidden Cost (From Blog Post 1)

```
The hidden cost of AI coding assistants isn't the license fee.

It's the 15-20 minutes your developers spend EVERY SESSION re-explaining the codebase to their AI.

Multiply that across your team. Across a month.

For a team of 10 developers, 3 sessions per day:
• 15 min × 3 sessions × 10 devs = 7.5 hours/day
• 37.5 hours/week
• 150+ hours/month

That's nearly a full-time engineer lost to context recovery.

The solution isn't better AI—it's better context management.

When decisions, patterns, and gotchas persist across sessions, your team stops repeating themselves.

3 signs your team has a context problem:
→ Same questions asked repeatedly to AI
→ Knowledge silos between team members
→ New hires ramping up slowly despite AI tools

What's your team's context recovery tax?
```

### Post 2: AI Productivity Paradox (From Blog Post 3)

```
I've noticed a pattern with teams using AI coding assistants:

Individual productivity goes up.
Team coordination goes down.

The symptoms:
→ PRs harder to review (AI code doesn't follow patterns)
→ Knowledge trapped in ephemeral chat sessions
→ Onboarding new devs actually gets harder

Here's the uncomfortable truth:

AI assistants were designed for individuals, not teams.

Each session is an island. There's no equivalent of git for AI context.

The fix isn't better AI. It's better knowledge management:

1. Capture decisions at the moment of discovery
2. Surface context automatically when starting work
3. Make AI-assisted work visible to the team

Engineering leaders: Are you tracking team coordination costs alongside AI productivity gains?

Measuring only individual output misses half the picture.
```

### Post 3: 15x Velocity (From Blog Post 6) - HIGHEST ENGAGEMENT POTENTIAL

```
We estimated 6-7 weeks.
We shipped in 3 days.

Not by working overtime. Not by cutting corners.

By eliminating coordination overhead.

Here's the math:

Traditional scrum team spends 40-50% of time on:
• Meetings (standups, planning, retros)
• Code review cycles
• Context switching
• Documentation and admin

Human+AI collaboration approaches 0% overhead:
• AI has instant codebase access (no sync needed)
• Decisions happen inline (no meetings)
• Context persists (no ramp-up)
• Quality is inline (no review cycles)

The result: 15x isn't magic. It's arithmetic.

But here's the uncomfortable question:

If one human+AI pair can outpace a small team, what's the role of the team?

We're still figuring this out. But early signals suggest teams shift from implementation collaboration to strategic alignment and review.

What velocity are you seeing with AI tools? Are you tracking it?
```

### Post 4: Measuring AI ROI (From Blog Post 5)

```
"How do we measure the ROI of AI coding tools?"

Every engineering leader is asking this question.

Most are measuring the wrong things.

Lines of code and commits per day measure OUTPUT.
They miss what AI actually changes about HOW work happens.

Better metrics:

1. Time to first commit
How long from task start to first meaningful commit?
AI dramatically affects context-loading time.

2. Rework rate
How often does code require changes after review?
AI can improve or hurt this—depends on workflow.

3. Knowledge propagation time
When one dev solves a problem, how fast does the solution spread?
This is the hidden cost of AI session isolation.

4. Context recovery time
How long to get productive after an interruption?
Track session starts—are devs re-explaining or jumping in?

The uncomfortable finding:

Individual productivity gains often mask team coordination losses.

A developer who's 30% more productive but creates knowledge silos that slow everyone else might be net negative.

Are you measuring both sides?
```

### Post 5: Onboarding Paradox

```
Here's a paradox we're seeing:

Teams adopt AI coding assistants.
Individual productivity improves.
But onboarding new developers gets HARDER.

Why?

Before AI: Tribal knowledge lived in code reviews, pair programming, hallway conversations. New hires absorbed it naturally.

After AI: Knowledge lives in ephemeral chat sessions. Each dev's AI has a different mental model. Nothing persists.

New hires now face:
→ Less pair programming (AI replaces some of it)
→ Less knowledge in PR discussions
→ More "ask the AI" instead of "ask a teammate"

The fix isn't abandoning AI. It's capturing context:

When a senior dev discovers a gotcha, it should be captured.
When a new dev starts a session, they should see it automatically.

AI should accelerate onboarding, not slow it down.

How's your team handling knowledge transfer in the AI era?
```

### Post 6: The Visibility Problem

```
As an engineering manager, you've always had visibility into work through:
→ Standups
→ PRs
→ Commit history

But AI-assisted work is increasingly invisible.

A developer might spend 2 hours in productive AI conversation, resulting in a perfectly-formed 50-line PR.

From outside, it looks like they wrote 50 lines.

You have no visibility into:
→ Alternatives considered
→ Decisions made along the way
→ Gotchas discovered
→ Reasoning behind the approach

For routine work, this is fine.

For architectural decisions? Complex debugging? Critical features?

The invisible reasoning is often more valuable than the final code.

We're building tools that make AI work observable without adding overhead to developers.

The goal: capture happens inline, not as a separate step.

That's the only way it actually gets done.

What visibility do you have into your team's AI-assisted work?
```

---

## Engagement Strategy

### Accounts to Follow & Engage With

**Categories:**
- Engineering leadership influencers
- DevRel at AI companies (Anthropic, OpenAI)
- Founder accounts of developer tools
- Engineering managers at target companies
- Tech leads sharing publicly

### How to Engage

**On leadership posts about AI/productivity:**
```
Add insight, not promotion:

"This resonates. We're seeing similar patterns—individual AI productivity
gains masking team coordination losses.

The key metric we've started tracking: time from 'teammate discovers
solution' to 'solution is available to everyone.'

In pre-AI world, this happened naturally through PR reviews and pair
programming. Now it requires intentional capture."
```

**On posts about engineering metrics:**
```
"Great framework. One metric I'd add: context recovery time.

How long does it take a dev to become productive after switching
tasks or starting a new session?

AI tools dramatically affect this—for better or worse depending
on context management."
```

### Relationship Building

- Engage consistently with same accounts
- Add value before asking for anything
- Share others' content with thoughtful additions
- Join relevant LinkedIn groups
- Comment substantively on leadership discussions

---

## Connection Strategy

### Target Profiles

| Role | Company Type | Priority |
|------|--------------|----------|
| Engineering Manager | Tech companies (100-1000 employees) | High |
| Director of Engineering | Mid-size tech | High |
| VP Engineering | Startups/Scale-ups | Medium |
| Tech Lead | AI-first companies | Medium |
| CTO | Startups | Medium |

### Connection Request Templates

**Template 1: Shared Interest**
```
Hi [Name],

Saw your post about [topic]. Really resonated with what we're
seeing in AI-assisted development—the coordination challenges
are real.

Would love to connect and exchange notes on how teams are
handling this.
```

**Template 2: Content Engagement**
```
Hi [Name],

I've been following your posts about engineering leadership.
Your take on [specific topic] was particularly insightful.

Connecting to follow along—we're thinking a lot about similar
challenges at ginko.
```

### After Connection

- Don't pitch immediately
- Engage with their content first
- Wait for natural opportunity to share relevant resource
- Build relationship before asking

---

## Posting Schedule

### Frequency

- **Target:** 2 posts/week
- **Days:** Tuesday, Thursday (highest engagement for B2B)
- **Time:** 8-10am or 12-1pm (target timezone)

### Monthly Calendar

| Week | Post 1 (Tue) | Post 2 (Thu) |
|------|--------------|--------------|
| 1 | 15x Velocity (Post 3) | AI Productivity Paradox (Post 2) |
| 2 | Measuring ROI (Post 4) | The Hidden Cost (Post 1) |
| 3 | Onboarding Paradox (Post 5) | Visibility Problem (Post 6) |
| 4 | New insight post | Engagement/reshare |

---

## Metrics & Goals

### Weekly Metrics

| Metric | Target |
|--------|--------|
| Posts published | 2 |
| Impressions | 1,000+ |
| Engagement rate | >3% |
| Profile views | 50+ |
| Connection requests sent | 10+ |
| Connections accepted | 5+ |

### Monthly Goals

| Metric | Month 1 | Month 2 | Month 3 |
|--------|---------|---------|---------|
| Total connections (target audience) | +20 | +40 | +60 |
| Avg post engagement rate | 2% | 3% | 4% |
| Inbound connection requests | 5 | 10 | 20 |
| Click-throughs to site | 20 | 50 | 100 |

---

## UTM Tracking

All links must include UTM parameters:

```
?utm_source=linkedin&utm_medium=organic-social&utm_campaign=sprint3-content&utm_content={content-type}-{topic}
```

**Examples:**
- Velocity post: `?utm_source=linkedin&utm_medium=organic-social&utm_campaign=sprint3-content&utm_content=post-velocity`
- ROI post: `?utm_source=linkedin&utm_medium=organic-social&utm_campaign=sprint3-content&utm_content=post-roi`

---

## Quick Reference

### Before Posting Checklist

- [ ] Hook in first line (people see only first 2 lines before "see more")
- [ ] Clear value for engineering leaders?
- [ ] Specific examples/data (not generic advice)?
- [ ] Soft CTA (question or link, not hard sell)?
- [ ] UTM tracking on any links?
- [ ] Proofread for typos?
- [ ] Ready to engage with comments?

### What Works on LinkedIn

- Personal stories and lessons learned
- Specific numbers and data
- Contrarian or surprising takes
- Frameworks and mental models
- Questions that invite discussion

### What Doesn't Work

- Generic advice ("work smarter not harder")
- Pure product promotion
- Long blocks of unformatted text
- No clear takeaway
- Hashtag stuffing

---

*Last updated: 2026-01-12*
*Sprint: EPIC-010 Sprint 3*
